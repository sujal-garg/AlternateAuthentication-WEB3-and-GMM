{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ys7SeectqDW"
      },
      "source": [
        "## Voice Authentication and Face Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lKxfAGBstqDi",
        "outputId": "e3a2233d-04e3-4def-e749-7971f198897c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyaudio\n",
            "  Using cached PyAudio-0.2.12.tar.gz (42 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (PEP 517) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\n",
            "Failed to build pyaudio\n",
            "\u001b[31mERROR: Could not build wheels for pyaudio which use PEP 517 and cannot be installed directly\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pipwin\n",
            "  Downloading pipwin-0.5.2.tar.gz (7.9 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pipwin) (2.23.0)\n",
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pipwin) (1.15.0)\n",
            "Collecting beautifulsoup4>=4.9.0\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 13.2 MB/s \n",
            "\u001b[?25hCollecting js2py\n",
            "  Downloading Js2Py-0.71-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pipwin) (21.3)\n",
            "Collecting pySmartDL>=1.3.1\n",
            "  Downloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Collecting pyjsparser>=2.5.1\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from js2py->pipwin) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal>=1.2->js2py->pipwin) (2022.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pipwin) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pipwin) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pipwin) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pipwin) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pipwin) (1.24.3)\n",
            "Building wheels for collected packages: pipwin, docopt, pyjsparser\n",
            "  Building wheel for pipwin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pipwin: filename=pipwin-0.5.2-py2.py3-none-any.whl size=8791 sha256=89a7a2ffbb568d1c1f62b76f9139377b70fab6d706e2edd4407f10be98b75d50\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/df/15/51b9c5d152e27b7fad998993eba50e15dfa709cc7438557f7e\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=3f93c72cfec25f93178776b9c2c0631200002ef8554a80fd3c6864ae8fbbdd28\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=26000 sha256=a5b04f2097216c08f4768c6d0c5361bbc5f89580c66e52b47e84b61bca4ca544\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/80/ac/dcd2bdbd03dd2b7b7e2bf3e5afbda6a1ab7935bbce314969da\n",
            "Successfully built pipwin docopt pyjsparser\n",
            "Installing collected packages: soupsieve, pyjsparser, pySmartDL, pyprind, js2py, docopt, beautifulsoup4, pipwin\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.11.1 docopt-0.6.2 js2py-0.71 pipwin-0.5.2 pySmartDL-1.3.4 pyjsparser-2.7.1 pyprind-2.11.3 soupsieve-2.3.2.post1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8734354b32cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pyaudio'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pipwin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyaudio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import cv2\n",
        "import time\n",
        "from numpy import genfromtxt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "K.set_image_data_format('channels_first')\n",
        "np.set_printoptions(threshold=np.inf, linewidth=np.nan)\n",
        "\n",
        "!pip install pyaudio\n",
        "!pip install pipwin\n",
        "import pyaudio\n",
        "from IPython.display import Audio, display, clear_output\n",
        "import wave\n",
        "from scipy.io.wavfile import read\n",
        "from sklearn.mixture import GMM \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn import preprocessing\n",
        "import python_speech_features as mfcc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4_SjN0gtqDm"
      },
      "source": [
        "## Facial Encodings\n",
        "\n",
        "The model provides output as 128 dim encoding vector for the input image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xk1k9skftqDn"
      },
      "outputs": [],
      "source": [
        "#provides 128 dim embeddings for face\n",
        "def img_to_encoding(img):\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    #converting img format to channel first\n",
        "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
        "\n",
        "    x_train = np.array([img])\n",
        "\n",
        "    #facial embedding from trained model\n",
        "    embedding = model.predict_on_batch(x_train)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRzfsNVFtqDo"
      },
      "source": [
        "## Triplet Loss\n",
        "\n",
        "Two encodings are compared and if they are similar then two images are of the same person otherwise they are different. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "k02jy3xetqDp",
        "outputId": "ed635c2f-8c73-4ca1-aaa0-f35b9f9e1b31"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-07cc36ffccb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'facenet_model/model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'triplet_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at facenet_model/model.h5"
          ]
        }
      ],
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "    \n",
        "    # triplet loss formula \n",
        "    pos_dist = tf.reduce_sum( tf.square(tf.subtract(y_pred[0], y_pred[1])) )\n",
        "    neg_dist = tf.reduce_sum( tf.square(tf.subtract(y_pred[0], y_pred[2])) )\n",
        "    basic_loss = pos_dist - neg_dist + alpha\n",
        "    \n",
        "    loss = tf.maximum(basic_loss, 0.0)\n",
        "   \n",
        "    return loss\n",
        "\n",
        "# load the model\n",
        "model = load_model('facenet_model/model.h5', custom_objects={'triplet_loss': triplet_loss})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5nGp1ittqDq"
      },
      "source": [
        "## Audio processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YvKpht-ftqDr"
      },
      "outputs": [],
      "source": [
        "#Calculate and returns the delta of given feature vector matrix\n",
        "def calculate_delta(array):\n",
        "    rows,cols = array.shape\n",
        "    deltas = np.zeros((rows,20))\n",
        "    N = 2\n",
        "    for i in range(rows):\n",
        "        index = []\n",
        "        j = 1\n",
        "        while j <= N:\n",
        "            if i-j < 0:\n",
        "                first = 0\n",
        "            else:\n",
        "                first = i-j\n",
        "            if i+j > rows -1:\n",
        "                second = rows -1\n",
        "            else:\n",
        "                second = i+j\n",
        "            index.append((second,first))\n",
        "            j+=1\n",
        "        deltas[i] = ( array[index[0][0]]-array[index[0][1]] + (2 * (array[index[1][0]]-array[index[1][1]])) ) / 10\n",
        "    return deltas\n",
        "\n",
        "#convert audio to mfcc features\n",
        "def extract_features(audio,rate):    \n",
        "    mfcc_feat = mfcc.mfcc(audio,rate, 0.025, 0.01,20,appendEnergy = True, nfft=1103)\n",
        "    mfcc_feat = preprocessing.scale(mfcc_feat)\n",
        "    delta = calculate_delta(mfcc_feat)\n",
        "\n",
        "    #combining both mfcc features and delta\n",
        "    combined = np.hstack((mfcc_feat,delta)) \n",
        "    return combined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhGLdLfNtqDs"
      },
      "source": [
        "## Add a New User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "wkCwEfuvtqDu",
        "outputId": "3f7a0bb8-5a73-4db9-a5cd-fbad9f75f711"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-543f6ebb409b>\"\u001b[0;36m, line \u001b[0;32m42\u001b[0m\n\u001b[0;31m    cv2_imshow(\"frame\" frame)\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def add_user():\n",
        "    \n",
        "    name = input(\"Enter Name:\")\n",
        "     # check for existing database\n",
        "    if os.path.exists('./face_database/embeddings.pickle'):\n",
        "        with open('./face_database/embeddings.pickle', 'rb') as database:\n",
        "            db = pickle.load(database)   \n",
        "            \n",
        "            if name in db or name == 'unknown':\n",
        "                print(\"Name Already Exists! Try Another Name...\")\n",
        "                return\n",
        "    else:\n",
        "        #if database not exists than creating new database\n",
        "        db = {}\n",
        "    \n",
        "    cap = cv2.VideoCapture(0)\n",
        "    cap.set(3, 640)\n",
        "    cap.set(4, 480)\n",
        "    \n",
        "    #detecting only frontal face using haarcascade\n",
        "    face_cascade = cv2.CascadeClassifier('./haarcascades/haarcascade_frontalface_default.xml')\n",
        "    \n",
        "    i = 3\n",
        "    face_found = False\n",
        "    \n",
        "    while True:            \n",
        "        _, frame = cap.read()\n",
        "        frame = cv2.flip(frame, 1, 0)\n",
        "            \n",
        "        #time.sleep(1.0)\n",
        "        cv2.putText(frame, 'Keep Your Face infront of Camera', (100, 200),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "        \n",
        "        cv2.putText(frame, 'Starting', (260, 270), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                    0.8, (255, 255, 255), 2)\n",
        "        \n",
        "        cv2.putText(frame, str(i), (290, 330), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                    1.3, (255, 255, 255), 3)\n",
        "\n",
        "        i-=1\n",
        "                   \n",
        "        cv2_imshow(\"frame\" frame)\n",
        "        cv2.waitKey(1000)\n",
        "        \n",
        "        if i < 0:\n",
        "            break\n",
        "            \n",
        "    start_time = time.time()        \n",
        "    img_path = './saved_image/1.jpg'\n",
        "\n",
        "    ## Face recognition \n",
        "    while True:\n",
        "        curr_time = time.time()\n",
        "        \n",
        "        _, frame = cap.read()\n",
        "        frame = cv2.flip(frame, 1, 0)\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        \n",
        "        face = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "        \n",
        "        if len(face) == 1:\n",
        "            for(x, y, w, h) in face:\n",
        "                roi = frame[y-10:y+h+10, x-10:x+w+10]\n",
        "\n",
        "                fh, fw = roi.shape[:2]\n",
        "\n",
        "                #make sure the face roi is of required height and width\n",
        "                if fh < 20 and fw < 20:\n",
        "                    continue\n",
        "\n",
        "                face_found = True\n",
        "                #cv2.imwrite(img_path, roi)\n",
        "\n",
        "                cv2.rectangle(frame, (x-10,y-10), (x+w+10, y+h+10), (255, 200, 200), 2)\n",
        "\n",
        "         \n",
        "        if curr_time - start_time >= 3:\n",
        "            break\n",
        "            \n",
        "        cv2.imshow('frame', frame)\n",
        "        cv2.waitKey(1)\n",
        "            \n",
        "    cap.release()        \n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    \n",
        "    if face_found:\n",
        "        img = cv2.resize(roi, (96, 96))\n",
        "\n",
        "        db[name] = img_to_encoding(img)\n",
        "\n",
        "        with open('./face_database/embeddings.pickle', \"wb\") as database:\n",
        "            pickle.dump(db, database, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "    elif len(face) > 1:\n",
        "        print(\"More than one faces found. Try again...\")\n",
        "        return\n",
        "    \n",
        "    else:\n",
        "        print('There was no face found in the frame. Try again...')\n",
        "        return\n",
        "      \n",
        "    clear_output(wait=True) \n",
        "    \n",
        "    #Voice authentication\n",
        "    FORMAT = pyaudio.paInt16\n",
        "    CHANNELS = 2\n",
        "    RATE = 44100\n",
        "    CHUNK = 1024\n",
        "    RECORD_SECONDS = 3\n",
        "    \n",
        "    source = \"./voice_database/\" + name\n",
        "    \n",
        "   \n",
        "    os.mkdir(source)\n",
        "\n",
        "    for i in range(3):\n",
        "        audio = pyaudio.PyAudio()\n",
        "\n",
        "        if i == 0:\n",
        "            j = 3\n",
        "            while j>=0:\n",
        "                time.sleep(1.0)\n",
        "                print(\"Speak your name in {} seconds\".format(j))\n",
        "                clear_output(wait=True)\n",
        "\n",
        "                j-=1\n",
        "\n",
        "        elif i ==1:\n",
        "            print(\"Speak your name one more time\")\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        else:\n",
        "            print(\"Speak your name one last time\")\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        # start Recording\n",
        "        stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
        "                    rate=RATE, input=True,\n",
        "                    frames_per_buffer=CHUNK)\n",
        "\n",
        "        print(\"recording...\")\n",
        "        frames = []\n",
        "\n",
        "        for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
        "            data = stream.read(CHUNK)\n",
        "            frames.append(data)\n",
        "\n",
        "        # stop Recording\n",
        "        stream.stop_stream()\n",
        "        stream.close()\n",
        "        audio.terminate()\n",
        "        \n",
        "        # saving wav file of speaker\n",
        "        waveFile = wave.open(source + '/' + str((i+1)) + '.wav', 'wb')\n",
        "        waveFile.setnchannels(CHANNELS)\n",
        "        waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
        "        waveFile.setframerate(RATE)\n",
        "        waveFile.writeframes(b''.join(frames))\n",
        "        waveFile.close()\n",
        "        print(\"Done\")\n",
        "\n",
        "    dest =  \"./gmm_models/\"\n",
        "    count = 1\n",
        "\n",
        "    for path in os.listdir(source):\n",
        "        path = os.path.join(source, path)\n",
        "\n",
        "        features = np.array([])\n",
        "        \n",
        "        # reading audio files of speaker\n",
        "        (sr, audio) = read(path)\n",
        "        \n",
        "        # extract 40 dimensional MFCC & delta MFCC features\n",
        "        vector   = extract_features(audio,sr)\n",
        "\n",
        "        if features.size == 0:\n",
        "            features = vector\n",
        "        else:\n",
        "            features = np.vstack((features, vector))\n",
        "            \n",
        "        # when features of 3 files of speaker are concatenated, then do model training\n",
        "        if count == 3:    \n",
        "            gmm = GMM(n_components = 16, n_iter = 200, covariance_type='diag',n_init = 3)\n",
        "            gmm.fit(features)\n",
        "\n",
        "            # saving the trained gaussian model\n",
        "            pickle.dump(gmm, open(dest + name + '.gmm', 'wb'))\n",
        "            print(name + ' added successfully') \n",
        "            \n",
        "            features = np.asarray(())\n",
        "            count = 0\n",
        "        count = count + 1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    add_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyjjZ0p5tqDw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxcVqxbCtqDx"
      },
      "source": [
        "## Delete User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Hbgs4tt3tqDz",
        "outputId": "a284bc32-cc1a-4809-f860-ea94f92f4ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter name of the user:sujal\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b4d542db6bca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such user !!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdelete_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-b4d542db6bca>\u001b[0m in \u001b[0;36mdelete_user\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter name of the user:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./face_database/embeddings.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './face_database/embeddings.pickle'"
          ]
        }
      ],
      "source": [
        "# deletes a registered user from database\n",
        "def delete_user():\n",
        "    name = input(\"Enter name of the user:\")\n",
        "    \n",
        "    with open(\"./face_database/embeddings.pickle\", \"rb\") as database:\n",
        "        db = pickle.load(database)\n",
        "        user = db.pop(name, None)\n",
        "    \n",
        "        if user is not None:\n",
        "            print('User ' + name + ' deleted successfully')\n",
        "            # save the database\n",
        "            with open('face_database/embeddings.pickle', 'wb') as database:\n",
        "                    pickle.dump(db, database, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            # remove the speaker wav files and gmm model\n",
        "            [os.remove(path) for path in glob.glob('./voice_database/' + name + '/*')]\n",
        "            os.removedirs('./voice_database/' + name)\n",
        "            os.remove('./gmm_models/' + name + '.gmm')\n",
        "        \n",
        "        else:\n",
        "            print('No such user !!')\n",
        "\n",
        "delete_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e_OO3axtqD0"
      },
      "source": [
        "## Voice Authentication and Face Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "1oDwCc_BtqD1",
        "outputId": "6b80fe29-3965-4f4c-d896-d3bdd821058a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-12a6e49474dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-12a6e49474dd>\u001b[0m in \u001b[0;36mrecognize\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Voice Authentication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mFORMAT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaInt16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mCHANNELS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mRATE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m44100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pyaudio' is not defined"
          ]
        }
      ],
      "source": [
        "def recognize():\n",
        "    # Voice Authentication\n",
        "    FORMAT = pyaudio.paInt16\n",
        "    CHANNELS = 2\n",
        "    RATE = 44100\n",
        "    CHUNK = 1024\n",
        "    RECORD_SECONDS = 3\n",
        "    FILENAME = \"./test.wav\"\n",
        "\n",
        "    audio = pyaudio.PyAudio()\n",
        "   \n",
        "    # start Recording\n",
        "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
        "                    rate=RATE, input=True,\n",
        "                    frames_per_buffer=CHUNK)\n",
        "\n",
        "    print(\"recording...\")\n",
        "    frames = []\n",
        "\n",
        "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
        "        data = stream.read(CHUNK)\n",
        "        frames.append(data)\n",
        "    print(\"finished recording\")\n",
        "\n",
        "\n",
        "    # stop Recording\n",
        "    stream.stop_stream()\n",
        "    stream.close()\n",
        "    audio.terminate()\n",
        "\n",
        "    # saving wav file \n",
        "    waveFile = wave.open(FILENAME, 'wb')\n",
        "    waveFile.setnchannels(CHANNELS)\n",
        "    waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
        "    waveFile.setframerate(RATE)\n",
        "    waveFile.writeframes(b''.join(frames))\n",
        "    waveFile.close()\n",
        "\n",
        "    modelpath = \"./gmm_models/\"\n",
        "\n",
        "    gmm_files = [os.path.join(modelpath,fname) for fname in \n",
        "                os.listdir(modelpath) if fname.endswith('.gmm')]\n",
        "\n",
        "    models    = [pickle.load(open(fname,'rb')) for fname in gmm_files]\n",
        "\n",
        "    speakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname \n",
        "                in gmm_files]\n",
        "  \n",
        "    if len(models) == 0:\n",
        "        print(\"No Users in the Database!\")\n",
        "        return\n",
        "        \n",
        "    #read test file\n",
        "    sr,audio = read(FILENAME)\n",
        "    \n",
        "    # extract mfcc features\n",
        "    vector = extract_features(audio,sr)\n",
        "    log_likelihood = np.zeros(len(models)) \n",
        "\n",
        "    #checking with each model one by one\n",
        "    for i in range(len(models)):\n",
        "        gmm = models[i]         \n",
        "        scores = np.array(gmm.score(vector))\n",
        "        log_likelihood[i] = scores.sum()\n",
        "\n",
        "    pred = np.argmax(log_likelihood)\n",
        "    identity = speakers[pred]\n",
        "   \n",
        "    # if voice not recognized than terminate the process\n",
        "    if identity == 'unknown':\n",
        "            print(\"Not Recognized! Try again...\")\n",
        "            return\n",
        "    \n",
        "    print( \"Recognized as - \", identity)\n",
        "\n",
        "    # face recognition\n",
        "    print(\"Keep Your face infront of the camera\")\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    cap.set(3, 640)\n",
        "    cap.set(4, 480)\n",
        "\n",
        "    cascade = cv2.CascadeClassifier('./haarcascades/haarcascade_frontalface_default.xml')\n",
        "    \n",
        "    #loading the database \n",
        "    database = pickle.load(open('face_database/embeddings.pickle', \"rb\"))\n",
        "    \n",
        "    time.sleep(1.0)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    while True:\n",
        "        curr_time = time.time()\n",
        "            \n",
        "        _, frame = cap.read()\n",
        "        frame = cv2.flip(frame, 1, 0)\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        face = cascade.detectMultiScale(gray, 1.3, 5)\n",
        "         \n",
        "        name = 'unknown'\n",
        "        \n",
        "        \n",
        "        if len(face) == 1:\n",
        "\n",
        "            for (x, y, w, h) in face:\n",
        "                roi = frame[y-10:y+h+10, x-10:x+w+10]\n",
        "            \n",
        "                fh, fw = roi.shape[:2]\n",
        "                min_dist = 100\n",
        "                \n",
        "                #make sure the face is of required height and width\n",
        "                if fh < 20 and fh < 20:\n",
        "                    continue\n",
        "\n",
        "                \n",
        "                #resizing image as required by the model\n",
        "                img = cv2.resize(roi, (96, 96))\n",
        "\n",
        "                #128 d encodings from pre-trained model\n",
        "                encoding = img_to_encoding(img)\n",
        "                \n",
        "                # loop over all the recorded encodings in database \n",
        "                for knownName in database:\n",
        "                    # find the similarity between the input encoding and \n",
        "                    #recorded encodings in database using L2 norm\n",
        "                    dist = np.linalg.norm(np.subtract(database[knownName], encoding) )\n",
        "                    \n",
        "                    # check if minimum distance or not\n",
        "                    if dist < min_dist:\n",
        "                        min_dist = dist\n",
        "                        name = knownName\n",
        "\n",
        "            # if min dist is less then threshold value and \n",
        "            #both face and voice matched than unlock the door\n",
        "            if min_dist < 0.4 and name == identity:\n",
        "                print (\"Door Unlocked! Welcome \" + str(name))\n",
        "                break\n",
        "\n",
        "        #open the cam for 3 seconds\n",
        "        if curr_time - start_time >= 3:\n",
        "            break    \n",
        "\n",
        "        cv2.waitKey(1)\n",
        "        cv2.imshow('frame', frame)\n",
        "        \n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "   \n",
        "    if len(face) == 0:\n",
        "        print('There was no face found in the frame. Try again...')\n",
        "        \n",
        "    elif len(face) > 1:\n",
        "        print(\"More than one faces found. Try again...\")\n",
        "        \n",
        "    elif min_dist > 0.5 or name != identity:\n",
        "        print(\"Not Recognized! Try again...\")\n",
        "   \n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    recognize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST2e44AltqD2"
      },
      "source": [
        "## Another version of recognizing user will keep runnning until KeyboardInterrupt by the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Y4AJX_YztqD3",
        "outputId": "d323555d-1caa-4828-db10-34e00619e965"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-299620bd68c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-299620bd68c6>\u001b[0m in \u001b[0;36mrecognize\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Voice Authentication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mFORMAT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaInt16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mCHANNELS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mRATE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m44100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pyaudio' is not defined"
          ]
        }
      ],
      "source": [
        "def recognize():\n",
        "    # Voice Authentication\n",
        "    FORMAT = pyaudio.paInt16\n",
        "    CHANNELS = 2\n",
        "    RATE = 44100\n",
        "    CHUNK = 1024\n",
        "    RECORD_SECONDS = 3\n",
        "    FILENAME = \"./test.wav\"\n",
        "    try:\n",
        "        while True:\n",
        "            audio = pyaudio.PyAudio()\n",
        "\n",
        "            # start Recording\n",
        "            stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
        "                            rate=RATE, input=True,\n",
        "                            frames_per_buffer=CHUNK)\n",
        "\n",
        "            print(\"recording...\")\n",
        "            frames = []\n",
        "\n",
        "            for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
        "                data = stream.read(CHUNK)\n",
        "                frames.append(data)\n",
        "            print(\"finished recording\")\n",
        "\n",
        "\n",
        "            # stop Recording\n",
        "            stream.stop_stream()\n",
        "            stream.close()\n",
        "            audio.terminate()\n",
        "\n",
        "            # saving wav file \n",
        "            waveFile = wave.open(FILENAME, 'wb')\n",
        "            waveFile.setnchannels(CHANNELS)\n",
        "            waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
        "            waveFile.setframerate(RATE)\n",
        "            waveFile.writeframes(b''.join(frames))\n",
        "            waveFile.close()\n",
        "\n",
        "            modelpath = \"./gmm_models/\"\n",
        "\n",
        "            gmm_files = [os.path.join(modelpath,fname) for fname in \n",
        "                        os.listdir(modelpath) if fname.endswith('.gmm')]\n",
        "\n",
        "            models    = [pickle.load(open(fname,'rb')) for fname in gmm_files]\n",
        "\n",
        "            speakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname \n",
        "                        in gmm_files]\n",
        "            \n",
        "            if len(models) == 0:\n",
        "                print(\"No Users Authorized!\")\n",
        "                break\n",
        "                \n",
        "            #read test file\n",
        "            sr,audio = read(FILENAME)\n",
        "\n",
        "            # extract mfcc features\n",
        "            vector = extract_features(audio,sr)\n",
        "            log_likelihood = np.zeros(len(models)) \n",
        "\n",
        "            #checking with each model one by one\n",
        "            for i in range(len(models)):\n",
        "                gmm = models[i]         \n",
        "                scores = np.array(gmm.score(vector))\n",
        "                log_likelihood[i] = scores.sum()\n",
        "\n",
        "            pred = np.argmax(log_likelihood)\n",
        "            identity = speakers[pred]\n",
        "\n",
        "            # if voice not recognized than terminate the process\n",
        "            if identity == 'unknown':\n",
        "                    print(\"Not Recognized! Try again...\")\n",
        "                    continue\n",
        "            \n",
        "            print( \"Recognized as - \", identity)\n",
        "\n",
        "             # face recognition\n",
        "            print(\"Keep Your face infront of the camera\")\n",
        "            cap = cv2.VideoCapture(0)\n",
        "            cap.set(3, 640)\n",
        "            cap.set(4, 480)\n",
        "            img_path = './saved_image/2.jpg'\n",
        "\n",
        "            cascade = cv2.CascadeClassifier(\n",
        "                        './haarcascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "            #loading the database \n",
        "            database = pickle.load(open('face_database/embeddings.pickle', \"rb\"))\n",
        "\n",
        "            time.sleep(1.0)\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            while True:\n",
        "                curr_time = time.time()\n",
        "\n",
        "                _, frame = cap.read()\n",
        "                frame = cv2.flip(frame, 1, 0)\n",
        "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                face = cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "                name = 'unknown'\n",
        "                \n",
        "                if len(face) == 1:\n",
        "\n",
        "                    for (x, y, w, h) in face:\n",
        "                        roi = frame[y-10:y+h+10, x-10:x+w+10]\n",
        "\n",
        "                        fh, fw = roi.shape[:2]\n",
        "                        min_dist = 100 \n",
        "\n",
        "                        #make sure the face is of required height and width\n",
        "                        if fh < 20 and fh < 20:\n",
        "                            continue\n",
        "\n",
        "                        #resizing image as required by the model\n",
        "                        img = cv2.resize(roi, (96, 96))\n",
        "        \n",
        "                        #128 d encodings from pre-trained model\n",
        "                        encoding = img_to_encoding(img)\n",
        "\n",
        "                        # loop over all the recorded encodings in database \n",
        "                        for knownName in database:\n",
        "                            # find the similarity between the input encodings \n",
        "                            # and recorded encodings in database using L2 norm\n",
        "                            dist = np.linalg.norm(np.subtract(database[knownName], encoding) )\n",
        "                            \n",
        "                            # check if minimum distance or not\n",
        "                            if dist < min_dist:\n",
        "                                min_dist = dist\n",
        "                                name = knownName\n",
        "\n",
        "                    # if min dist is less then threshold value \n",
        "                    # and both face and voice matched than unlock the door\n",
        "                    if min_dist < 0.4 and name == identity:\n",
        "                        print (\"Door Unlocked! Welcome \" + str(name))\n",
        "                        break\n",
        "\n",
        "                #open the cam for 3 seconds\n",
        "                if curr_time - start_time >= 3:\n",
        "                    break\n",
        "\n",
        "                cv2.waitKey(1)\n",
        "                cv2.imshow('frame', frame)\n",
        "\n",
        "            cap.release()\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "            if len(face) == 0:\n",
        "                print('There was no face found in the frame. Try again...')\n",
        "                continue\n",
        "\n",
        "            elif len(face) > 1:\n",
        "                print(\"More than one faces found. Try again...\")\n",
        "                continue\n",
        "\n",
        "            elif min_dist > 0.4 or name != identity:\n",
        "                print(\"Not Recognized! Try again...\")\n",
        "                continue\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Stopped\")\n",
        "        pass\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    recognize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QawbTLAHtqD4"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn import mixture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-MZkAXcttqD5",
        "outputId": "55c077e0-9aef-4204-83b4-cb5a0ec45a9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "sklearn.__version__ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "qqYieeRUvIqb",
        "outputId": "1d4e97b3-12dc-4085-be5a-a26ff0ed84e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mephemeral\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       readonly=readonly)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 125\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}